<!doctype html>
<html lang="zh-CN">
    <head>
        <meta charset="ansi">
	<meta name="description" content="this is a website">
        <meta name="keywords" content="website,html,css">

        <title>ReadingGroup</title>

        <link rel="stylesheet" style="text/css" href="index.css">

    </head>
    <body>
        <div id="head">
            <div class="logo_title">
                <h1 style = "text-align:center"><font face="verdana" color="#0D1E91">Deep Robust & Explainable AI Lab Reading Group</font></h1>
          </div>
            <div class="clearfloat"></div>
        </div>
		
		<div id="wrapper">
            <div class="main">

				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2022-04-05</font></h2>
							<h3><a href="https://openreview.net/forum?id=S9ZyhWC17wJ"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Learning with Noisy Correspondence for Cross-modal Matching								
							</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted by: NIPS-2021 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng Ma</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 12:30-2:00 p.m. EDT</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/97832270671"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/97832270671</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1V_N8T6WlwIEyzVMxVaW1y3bFizs_0XNaXcuCb0JVibY/edit#slide=id.p"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						Cross-modal matching, which aims to establish the correspondence between two different modalities, is fundamental to a variety of tasks such as cross-modal retrieval and vision-and-language understanding. Although a huge number of cross-modal matching methods have been proposed and achieved remarkable progress in recent years, almost all of these methods implicitly assume that the multimodal training data are correctly aligned. In practice, however, such an assumption is extremely expensive even impossible to satisfy. Based on this observation, we reveal and study a latent and challenging direction in cross-modal matching, named noisy correspondence, which could be regarded as a new paradigm of noisy labels. Different from the traditional noisy labels which mainly refer to the errors in category labels, our noisy correspondence refers to the mismatch paired samples. To solve this new problem, we propose a novel method for learning with noisy correspondence, named Noisy Correspondence Rectifier (NCR). In brief, NCR divides the data into clean and noisy partitions based on the memorization effect of neural networks and then rectifies the correspondence via an adaptive prediction model in a co-teaching manner. To verify the effectiveness of our method, we conduct experiments by using the image-text matching as a showcase. Extensive experiments on Flickr30K, MS-COCO, and Conceptual Captions verify the effectiveness of our method. The code could be accessed from www.pengxi.me .
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2022-03-22</font></h2>
							<h3><a href="https://arxiv.org/abs/2106.10731"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Neighborhood Contrastive Learning for Novel Class Discovery								
							</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted by: CVPR-2021 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Qitong Wang</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 12:30-2:00 p.m. EDT</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/97832270671"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/97832270671</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1gp2mD7F03u6rWAnjU4kFkazU2AxLVCtZmeVx_4Tr8GI/edit#slide=id.g11a7c776993_0_0"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						In this paper, we address Novel Class Discovery (NCD), the task of unveiling new classes in a set of unlabeled samples given a labeled dataset with known classes. We exploit the peculiarities of NCD to build a new framework, named Neighborhood Contrastive Learning (NCL), to learn discriminative representations that are important to clustering performance. Our contribution is twofold. First, we find that a feature extractor trained on the labeled set generates representations in which a generic query sample and its neighbors are likely to share the same class. We exploit this observation to retrieve and aggregate pseudo-positive pairs with contrastive learning, thus encouraging the model to learn more discriminative representations. Second, we notice that most of the instances are easily discriminated by the network, contributing less to the contrastive loss. To overcome this issue, we propose to generate hard negatives by mixing labeled and unlabeled samples in the feature space. We experimentally demonstrate that these two ingredients significantly contribute to clustering performance and lead our model to outperform state-of-the-art methods by a large margin (e.g., clustering accuracy +13% on CIFAR-100 and +8% on ImageNet).
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2022-03-15</font></h2>
							<h3><a href="https://arxiv.org/abs/2003.11163"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Fusing Wearable IMUs with Multi-View Images for Human Pose Estimation: A Geometric Approach								
							</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted by: CVPR-2020 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ziyang Jia</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 12:30-2:00 p.m. EDT</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/97832270671"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/97832270671</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/16rFKxB86gHXPSjncUA7QgZ_ntwxXlbmxzvRIXN1dpW8/edit#slide=id.g11d408adddf_0_0"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						We propose to estimate 3D human pose from multi-view images and a few IMUs attached at person's limbs. It operates by firstly detecting 2D poses from the two signals, and then lifting them to the 3D space. We present a geometric approach to reinforce the visual features of each pair of joints based on the IMUs. This notably improves 2D pose estimation accuracy especially when one joint is occluded. We call this approach Orientation Regularized Network (ORN). Then we lift the multi-view 2D poses to the 3D space by an Orientation Regularized Pictorial Structure Model (ORPSM) which jointly minimizes the projection error between the 3D and 2D poses, along with the discrepancy between the 3D pose and IMU orientations. The simple two-step approach reduces the error of the state-of-the-art by a large margin on a public dataset.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2022-03-01</font></h2>
							<h3><a href="https://arxiv.org/abs/1912.06430"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> End-to-End Learning of Visual Representations from Uncurated Instructional Videos
							</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted by: CVPR-2020 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Amani Arman Kiruga</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 12:30-2:00 p.m. EST</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/97832270671"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/97832270671</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1nxZJFha2mSBtwyQ3ydbR1j7ATzg_827NsRwG_4Zp3W8/edit#slide=id.p"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						Annotating videos is cumbersome, expensive and not scalable. Yet, many strong video models still rely on manually annotated data. With the recent introduction of the HowTo100M dataset, narrated videos now offer the possibility of learning video representations without manual supervision. In this work we propose a new learning approach, MIL-NCE, capable of addressing misalignments inherent to narrated videos. With this approach we are able to learn strong video representations from scratch, without the need for any manual annotation. We evaluate our representations on a wide range of four downstream tasks over eight datasets: action recognition (HMDB-51, UCF-101, Kinetics-700), text-to-video retrieval (YouCook2, MSR-VTT), action localization (YouTube-8M Segments, CrossTask) and action segmentation (COIN). Our method outperforms all published self-supervised approaches for these tasks as well as several fully supervised baselines.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2022-02-22</font></h2>
							<h3><a href="https://arxiv.org/abs/2003.08934"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
							</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted by: ECCV-2020 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Nathaniel Merrill</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 12:30-2:00 p.m. EST</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/97832270671"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/97832270671</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1Q2ds__PRkiabFmTtP3pCePOBs4a2b7cGBssEwFeqAD8/edit#slide=id.p"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,y,z) and viewing direction (θ,ϕ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2022-02-15</font></h2>
							<h3><a href="https://arxiv.org/abs/2105.01937"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> FLEX: Parameter-free Multi-view 3D Human Motion Reconstruction
							</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Submitted to: arXiv-2021 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Kien Nguyen</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 1:00-2:00 p.m. EST</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/97832270671"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/97832270671</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/19xkwAkChKMrqkiNl4WQnYKcru72uKwaa5duCm1s2wKo/edit#slide=id.p"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						The increasing availability of video recordings made by multiple cameras has offered new means for mitigatingocclusion and depth ambiguities in pose and motion reconstruction methods. Yet, multi-view algorithms strongly depend on camera parameters; particularly, the relativepositions between the cameras. Such a dependency becomes a hurdle once shifting to dynamic capture in uncontrolled settings. We introduce FLEX (Free muLti-view rEconstruXion), an end-to-end parameter-free multi-viewmodel. FLEX is parameter-free in the sense that it does not require any camera parameters, neither intrinsic nor extrinsic. Our key idea is that the 3D angles between skeletal parts, as well as bone lengths, are invariant to the camera position. Hence, learning 3D rotations and bone lengths rather than locations allows predicting common values for all camera views. Our network takes multiple video streams, learns fused deep features through a novel multi-view fusion layer, and reconstructs a single consistent skeleton with temporally coherent joint rotations. We demonstrate quantitative and qualitative results on the Human3.6M and KTH Multi-view Football II datasets, and on synthetic multi-person video streams captured by dynamic cameras. We compare our model to state-of-the-art methods that are not parameter-free and show that in the absence of camera parameters, we outperform them by a large margin while obtaining comparable results when camera parameters are available. Code, trained models, video examples, and more material will be available on our project page.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2022-02-08</font></h2>
							<h3><a href="https://www.researchgate.net/publication/343744565_A_method_to_evaluate_task-specific_importance_of_spatio-temporal_units_based_on_explainable_artificial_intelligence"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> A method to evaluate task-specific importance of spatio-temporal units based on explainable artificial intelligence
							</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted by: International Journal of Geographical Information Science, 2020. </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Tang Li</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 12:30-2:00 p.m. EST</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/97832270671"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/97832270671</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/14zqPfNwV-YTbJqy6bC3RyPSU9F5itp6NhVTOUVX4oZw/edit"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						Big geo-data are often aggregated according to spatio-temporal units for analyzing human activities and urban environments. Many applications categorize such data into groups and compare the characteristics across groups. The intergroup differences vary with spatio-temporal units, and the essential is to identify the spatio-temporal units with apparently different data characteristics. However, spatio-temporal dependence, data variety, and the complexity of tasks impede an effective unit assessment. Inspired by the applications to extract critical image components based on explainable artificial intelligence (XAI), we propose a spatio-temporal layer-wise relevance propagation method to assess spatio-temporal units as a general solution. The method organizes input data into an extensible three-dimensional tensor form. We provide two means of labeling the spatio-temporal tensor data for typical geographical applications, using temporally or spatially relevant information. Neural network training proceeds to extract the global and local characteristics of data for corresponding analytical tasks. Then the method propagates classification results backward into units as obtained task-specific importance. A case study with taxi trajectory data in Beijing validates the method. The results prove that the proposed method can evaluate the task-specific importance of spatio-temporal units with dependence. This study also attempts to discover task-related knowledge using XAI.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2022-02-03</font></h2>
							<h3><a href="https://arxiv.org/abs/2012.04324"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Continual Adaptation of Visual Representations via Domain Randomization and Meta-learning
							</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted by: CVPR-2021 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Fengchun Qiao</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 7:00-8:00 p.m. EST</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/97832270671"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/97832270671</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1oadhV8Ph-Fc6V_psDgF7EwJOX2HOYbIx/edit"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						Most standard learning approaches lead to fragile models which are prone to drift when sequentially trained on samples of a different nature - the well-known "catastrophic forgetting" issue. In particular, when a model consecutively learns from different visual domains, it tends to forget the past domains in favor of the most recent ones. In this context, we show that one way to learn models that are inherently more robust against forgetting is domain randomization - for vision tasks, randomizing the current domain's distribution with heavy image manipulations. Building on this result, we devise a meta-learning strategy where a regularizer explicitly penalizes any loss associated with transferring the model from the current domain to different "auxiliary" meta-domains, while also easing adaptation to them. Such meta-domains are also generated through randomized image manipulations. We empirically demonstrate in a variety of experiments - spanning from classification to semantic segmentation - that our approach results in models that are less prone to catastrophic forgetting when transferred to new domains.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2022-01-27</font></h2>
							<h3><a href="https://proceedings.neurips.cc/paper/2021/hash/018b59ce1fd616d874afad0f44ba338d-Abstract.html"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Non-Local Latent Relation Distillation for Self-Adaptive 3D Human Pose Estimation</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted by: NIPS-2021 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng Ma</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 7:00-8:00 p.m. EST</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/97832270671"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/97832270671</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1E-cIcA2OVZuW_WyagWF_-UaeajYjIjQIb3OjjCu79qs/edit#slide=id.p"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						Available 3D human pose estimation approaches leverage different forms of strong (2D/3D pose) or weak (multi-view or depth) paired supervision. Barring synthetic or in-studio domains, acquiring such supervision for each new target environment is highly inconvenient. To this end, we cast 3D pose learning as a self-supervised adaptation problem that aims to transfer the task knowledge from a labeled source domain to a completely unpaired target. We propose to infer image-to-pose via two explicit mappings viz. image-to-latent and latent-to-pose where the latter is a pre-learned decoder obtained from a prior-enforcing generative adversarial auto-encoder. Next, we introduce relation distillation as a means to align the unpaired cross-modal samples i.e., the unpaired target videos and unpaired 3D pose sequences. To this end, we propose a new set of non-local relations in order to characterize long-range latent pose interactions, unlike general contrastive relations where positive couplings are limited to a local neighborhood structure. Further, we provide an objective way to quantify non-localness in order to select the most effective relation set. We evaluate different self-adaptation settings and demonstrate state-of-the-art 3D human pose estimation performance on standard benchmarks.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2022-01-20</font></h2>
							<h3><a href="https://arxiv.org/abs/2104.07905"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Ego-Exo: Transferring Visual Representations from Third-person to First-person Videos</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted by: CVPR-2021 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Qitong Wang</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 7:00-8:00 p.m. EST</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/97832270671"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/97832270671</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1iDAAh0OEfU02-vffYDxcLsXWNK4C05nymx8oIcCiH3E/edit#slide=id.p"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						We introduce an approach for pre-training egocentric video models using large-scale third-person video datasets. Learning from purely egocentric data is limited by low dataset scale and diversity, while using purely exocentric (third-person) data introduces a large domain mismatch. Our idea is to discover latent signals in third-person video that are predictive of key egocentric-specific properties. Incorporating these signals as knowledge distillation losses during pre-training results in models that benefit from both the scale and diversity of third-person video data, as well as representations that capture salient egocentric properties. Our experiments show that our Ego-Exo framework can be seamlessly integrated into standard video models; it outperforms all baselines when fine-tuned for egocentric activity recognition, achieving state-of-the-art results on Charades-Ego and EPIC-Kitchens-100.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2021-12-08</font></h2>
							<h3><a href="https://docs.google.com/presentation/d/1ITkb_qmfnm23buhqVwLfE6BzGiBLgJS_pQrU13NjuZI/edit#slide=id.g10696af11ea_1_104"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> A Recent Trend on Contrastive Learning</font></a></h3>
							<!-- <p class="item_info"><font face="verdana" color="black" size="2.5">Submitted to: arXiv-2020 </font></p> -->
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Zhenzhu Zheng</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 8:00-9:00 p.m. EST</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/95652916677"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/95652916677</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1ITkb_qmfnm23buhqVwLfE6BzGiBLgJS_pQrU13NjuZI/edit#slide=id.g10696af11ea_1_104"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						Contrastive Learning has recently received interest due to its rapid success in self-supervised representation learning. This presentation first reviews the recent work in the area of contrastive learning. Although contrastive learning methods show significant progress on large model training, it does not work well for small models. This brings us to another area known as Knowledge Distillation. Although most previous studies on knowledge distillation are in supervised settings, this presentation further discusses the recent trend of knowledge distillation merged with contrast learning. This motivates the key questions of the future as to how to learn so much from observation alone. Finally, we discuss possible open problems.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2021-12-01</font></h2>
							<h3><a href="https://arxiv.org/pdf/2002.00569.pdf"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> DiverseDepth: Affine-invariant Depth Prediction Using Diverse Data</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Submitted to: arXiv-2020 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Nathaniel Merrill</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 8:00-9:00 p.m. EST</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/95652916677"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/95652916677</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1SRKriEigG_GuaSBZjl6zBEhrT3B9S4cQ5idDT_WXEuI/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						We present a method for depth estimation with monocular images, which can predict high-quality depth on diverse scenes up to an affine transformation, thus preserving accurate shapes of a scene. Previous methods that predict metric depth often work well only for a specific scene. In contrast, learning relative depth (information of being closer or further) can enjoy better generalization, with the price of failing to recover the accurate geometric shape of the scene. In this work, we propose a dataset and methods to tackle this dilemma, aiming to predict accurate depth up to an affine transformation with good generalization to diverse scenes. First we construct a large-scale and diverse dataset, termed Diverse Scene Depth dataset (DiverseDepth), which has a broad range of scenes and foreground contents. Compared with previous learning objectives, i.e., learning metric depth or relative depth, we propose to learn the affine-invariant depth using our diverse dataset to ensure both generalization and high-quality geometric shapes of scenes. Furthermore, in order to train the model on the complex dataset effectively, we propose a multi-curriculum learning method. Experiments show that our method outperforms previous methods on 8 datasets by a large margin with the zero-shot test setting, demonstrating the excellent generalization capacity of the learned model to diverse scenes. The reconstructed point clouds with the predicted depth show that our method can recover high-quality 3D shapes. Code and dataset are available at: https://tinyurl.com/DiverseDepth
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2021-11-17</font></h2>
							<h3><a href="https://docs.google.com/presentation/d/1rpwGxJ0jWBs16n3-U9r7J8UkmlKNkYEzclrzvqqZUSg/edit#slide=id.p"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Survey on Out-of-Domain Detection in Deep Learning</font></a></h3>
							<!-- <p class="item_info"><font face="verdana" color="black" size="2.5">Accepted by: CVPR-2021 </font></p> -->
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Wenxuan Li</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 7:00-8:00 p.m. EST</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/95652916677"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/95652916677</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1rpwGxJ0jWBs16n3-U9r7J8UkmlKNkYEzclrzvqqZUSg/edit#slide=id.p"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						In this survey, we focus on the out-of-domain detection which in other words, covariate shift detection. First, started with a broad wide topic “out-of-distribution detection” by given a unified framework termed generalized out-of-distribution detection, since out-of-domain detection is a subtask of out-of-distribution detection. Under the given framework, the five problems which include Anomaly Detection, Novelty Detection, Open Set Recognition, Out-of-Distribution Detection, and Outlier Detection, can be viewed as special cases or subtopics. Then clarified the background, definition, and application of each subtopic. Since these five subtopics could be categorized by whether occurs covariate shift or semantic shift or both, and currently, we are only interested in the covariate shift detection. Therefore, we selected the subtopics occurred covariate shift, summarized recent technical developments and categorized existing methods of each subtopic. At the end, a comprehensive paper list of all five subtopics is given.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2021-11-10</font></h2>
							<h3><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Fan_Point_4D_Transformer_Networks_for_Spatio-Temporal_Modeling_in_Point_Cloud_CVPR_2021_paper.pdf"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Point 4D Transformer Networks for Spatio-Temporal Modeling in Point Cloud Videos</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted by: CVPR-2021 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Shivanand Venkanna Sheshappanavar</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 8:00-9:00 p.m. EST</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/95652916677"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/95652916677</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="P4Transformer.pptx"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						Point cloud videos exhibit irregularities and lack of order along the spatial dimension where points emerge inconsistently across different frames. To capture the dynamics in point cloud videos, point tracking is usually employed. However, as points may flow in and out across frames, computing accurate point trajectories is extremely difficult. Moreover, tracking usually relies on point colors and thus may fail to handle colorless point clouds. In this paper, to avoid point tracking, we propose a novel Point 4D Transformer (P4Transformer) network to model raw point cloud videos. Specifically, P4Transformer consists of (i) a point 4D convolution to embed the spatio-temporal local structures presented in a point cloud video and (ii) a transformer to capture the appearance and motion information across the entire video by performing self-attention on the embedded local features. In this fashion, related or similar local areas are merged with attention weight rather than by explicit tracking. Extensive experiments, including 3D action recognition and 4D semantic segmentation, on four benchmarks demonstrate the effectiveness of our P4Transformer for point cloud video modeling.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2021-10-27</font></h2>
							<h3><a href="https://arxiv.org/pdf/2103.15691.pdf"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> ViViT: A Video Vision Transformer</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted by: ICCV-2021 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Kien Nguyen</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 8:00-9:00 p.m. EDT</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/95652916677"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/95652916677</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. To facilitate further research, we will release code and models.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2021-10-20</font></h2>
							<h3><a href="https://www.nature.com/articles/s41598-020-75167-6"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Volumetric Breast Density Estimation on MRI Using Explainable Deep Learning Regression</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted by: Nature: Scientific Reports-2021 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Tang Li</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 8:00-9:00 p.m. EDT</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/95652916677"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/95652916677</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						To purpose of this paper was to assess the feasibility of volumetric breast density estimations on MRI without segmentations accompanied with an explainability step. A total of 615 patients with breast cancer were included for volumetric breast density estimation. A 3-dimensional regression convolutional neural network (CNN) was used to estimate the volumetric breast density. Patients were split in training (N = 400), validation (N = 50), and hold-out test set (N = 165). Hyperparameters were optimized using Neural Network Intelligence and augmentations consisted of translations and rotations. The estimated densities were evaluated to the ground truth using Spearman’s correlation and Bland–Altman plots. The output of the CNN was visually analyzed using SHapley Additive exPlanations (SHAP). Spearman’s correlation between estimated and ground truth density was ρ = 0.81 (N = 165, P < 0.001) in the hold-out test set. The estimated density had a median bias of 0.70% (95% limits of agreement = − 6.8% to 5.0%) to the ground truth. SHAP showed that in correct density estimations, the algorithm based its decision on fibroglandular and fatty tissue. In incorrect estimations, other structures such as the pectoral muscle or the heart were included. To conclude, it is feasible to automatically estimate volumetric breast density on MRI without segmentations, and to provide accompanying explanations.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2021-10-13</font></h2>
							<h3><a href="https://openreview.net/forum?id=EKV158tSfwv"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Efficient Continual Learning with Modular Networks and Task-Driven Prior</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Submitted to: ICLR-2021 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Fengchun Qiao</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 8:00-9:00 p.m. EDT</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/95652916677"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/95652916677</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						Existing literature in Continual Learning (CL) has focused on overcoming catastrophic forgetting, the inability of the learner to recall how to perform tasks observed in the past. 
						There are however other desirable properties of a CL system, such as the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. Since most current benchmarks focus only on forgetting using short streams of tasks, we first propose a new suite of benchmarks to probe CL algorithms across these new axes. 
						Finally, we introduce a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. Learning a task reduces to figuring out which past modules to re-use, and which new modules to instantiate to solve the current task. Our learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks. 
						Our experiments show that this modular architecture and learning algorithm perform competitively on widely used CL benchmarks while yielding superior performance on the more challenging benchmarks we introduce in this work. The Benchmark is publicly available at https://github.com/facebookresearch/CTrLBenchmark.
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2021-10-06</font></h2>
							<h3><a href="https://arxiv.org/abs/2102.00529"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Submitted to: ArXiv-2021 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng Ma</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 8:00-9:00 p.m. EDT</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/95652916677"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/95652916677</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						Recently multimodal transformer models have gained popularity because their performance on language and vision tasks suggest they learn rich visual-linguistic representations. Focusing on zero-shot image retrieval tasks, we study three important factors which can impact the quality of learned representations: pretraining data, the attention mechanism, and loss functions. By pretraining models on six datasets, we observe that dataset noise and language similarity to our downstream task are important indicators of model performance. Through architectural analysis, we learn that models with a multimodal attention mechanism can outperform deeper models with modality specific attention mechanisms. Finally, we show that successful contrastive losses used in the self-supervised learning literature do not yield similar performance gains when used in multimodal transformers
					</div> </div>
				
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2021-09-29</font></h2>
							<h3><a href="https://openreview.net/pdf?id=rJeZq4reLS"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Time-series Generative Adversarial Networks</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: NIPS-2019 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Pranjal Dhakal</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 8:00-9:00 p.m. EDT</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/95652916677"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/95652916677</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						A good generative model for time-series data should preserve temporal dynamics, in the sense that new sequences respect the original relationships between variables across time. Existing methods that bring generative adversarial networks (GANs) into the sequential setting do not adequately attend to the temporal correlations unique to time-series data. At the same time, supervised models for sequence prediction - which allow finer control over network dynamics - are inherently deterministic. We propose a novel framework for generating realistic time-series data that combines the flexibility of the unsupervised paradigm with the control afforded by supervised training. Through a learned embedding space jointly optimized with both supervised and adversarial objectives, we encourage the network to adhere to the dynamics of the training data during sampling. Empirically, we evaluate the ability of our method to generate realistic samples using a variety of real and synthetic time-series datasets. Qualitatively and quantitatively, we find that the proposed framework consistently and significantly outperforms state-of-the-art benchmarks with respect to measures of similarity and predictive ability.
					</div> </div>
		
				<div class="item">
					<div class="item_content">
							<h2><font face="verdana" color="black" size="5">2021-09-22</font></h2>
							<h3><a href="https://arxiv.org/pdf/2103.16559.pdf"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Broaden Your Views for Self-Supervised Video Learning</font></a></h3>
							<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICCV-2021 </font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Amani Arman Kiruga</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 8:00-9:00 p.m. EDT</font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/95652916677"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/95652916677</font></a></font></p>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
				<p class="item_descri">
					<font face="verdana" color="black" size="2.5">	
						Most successful self-supervised learning methods are trained to align the representations of two independent views from the data. State-of-the-art methods in video are inspired by image techniques, where these two views are similarly extracted by cropping and augmenting the resulting crop. However, these methods miss a crucial element in the video domain: time. We introduce BraVe, a self-supervised learning framework for video. In BraVe, one of the views has access to a narrow temporal window of the video while the other view has a broad access to the video content. Our models learn to generalise from the narrow view to the general content of the video. Furthermore, BraVe processes the views with different backbones, enabling the use of alternative augmentations or modalities into the broad view such as optical flow, randomly convolved RGB frames, audio or their combinations. We demonstrate that BraVe achieves state-of-the-art results in self-supervised representation learning on standard video and audio classification benchmarks including UCF101, HMDB51, Kinetics, ESC-50 and AudioSet.
					</div> </div>
		    
        <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2021-09-15</font></h2>
					<h3><a href="https://arxiv.org/pdf/2006.12013.pdf"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information</font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICML-2020 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Qitong Wang</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 8:00-9:00 p.m. EDT</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/95652916677"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/95652916677</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">	
				Mutual information (MI) minimization has gained considerable interests in various machine learning tasks. However, estimating and minimizing MI in high-dimensional spaces remains a challenging problem, especially when only samples, rather than distribution forms, are accessible. Previous works mainly focus on MI lower bound approximation, which is not applicable to MI minimization problems. In this paper, we propose a novel Contrastive Log-ratio Upper Bound (CLUB) of mutual information. We provide a theoretical analysis of the properties of CLUB and its variational approximation. Based on this upper bound, we introduce a MI minimization training scheme and further accelerate it with a negative sampling strategy. Simulation studies on Gaussian distributions show the reliable estimation ability of CLUB. Real-world MI minimization experiments, including domain adaptation and information bottleneck, demonstrate the effectiveness of the proposed method. The code is at https://github.com/Linear95/CLUB.
	</div> </div>

	<div class="item">
		<div class="item_content">
				<h2><font face="verdana" color="black" size="5">2021-05-12</font></h2>
				<h3><a href="https://arxiv.org/abs/1906.05849"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Contrastive Multiview Coding</font></a></h3>
				<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ArXiv'19 </font></p>
	<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Qitong </font></p>
	<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
	<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
	<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
	<p class="item_descri">
		<font face="verdana" color="black" size="2.5">	
		Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a "dog" can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Our approach achieves state-of-the-art results on image and video unsupervised learning benchmarks.
</div> </div>
	
       <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2021-05-05</font></h2>
					<h3><a href="https://arxiv.org/pdf/1807.10165.pdf"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> UNet++: A Nested U-Net Architecturefor Medical Image Segmentation</font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: 4th Deep Learning in Medical Image Analysis (DLMIA) Workshop </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Tang </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">	
			In this paper, we present UNet++, a new, more powerful ar-chitecture for medical image segmentation. Our architecture is essentiallya deeply-supervised encoder-decoder network where the encoder and de-coder sub-networks are connected through a series of nested, dense skippathways. The re-designed skip pathways aim at reducing the semanticgap between the feature maps of the encoder and decoder sub-networks.We argue that the optimizer would deal with an easier learning task whenthe feature maps from the decoder and encoder networks are semanticallysimilar. We have evaluated UNet++ in comparison with U-Net and wideU-Net architectures across multiple medical image segmentation tasks:nodule segmentation in the low-dose CT scans of chest, nuclei segmen-tation  in  the  microscopy  images,  liver  segmentation  in  abdominal  CTscans, and polyp segmentation in colonoscopy videos. Our experimentsdemonstrate  that  UNet++  with  deep  supervision  achieves  an  averageIoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.
	</div> </div>		    
		    
		    
      <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2021-04-21</font></h2>
					<h3><a href="https://papers.nips.cc/paper/2017/file/21c5bba1dd6aed9ab48c2b34c1a0adde-Paper.pdf"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Few-Shot Adversarial Domain Adaptation</font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: NIPS 2017 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Pranjal </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">	
			This work provides a framework for addressing the problem of supervised domainadaptation with deep models. The main idea is to exploit adversarial learning tolearn an embedded subspace that simultaneously maximizes the confusion betweentwo domains while semantically aligning their embedding. The supervised settingbecomes attractive especially when there are only a few target data samples thatneed to be labeled. In thisfew-shot learningscenario, alignment and separation ofsemantic probability distributions is difficult because of the lack of data. We foundthat by carefully designing a training scheme whereby the typical binary adversarialdiscriminator is augmented to distinguish between four different classes,  it ispossible to effectively address the supervised adaptation problem. In addition, theapproach has a high “speed” of adaptation, i.e. it requires an extremely low numberof labeled target training samples, even one per category can be effective. We thenextensively compare this approach to the state of the art in domain adaptation intwo experiments:  one using datasets for handwritten digit recognition, and oneusing datasets for visual object recognition.
    	</div> </div>
		    
		    
	
		    
       <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2021-04-14</font></h2>
					<h3><a href="https://arxiv.org/abs/2008.01065"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Memory-augmented Dense Predictive Coding for Video Representation Learning</font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ECCV 2020 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ziyang </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/95652916677"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/95652916677</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <font face="verdana" color="red" size="2.5">Coming soon...</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">	
			The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condense representations, allowing to make multiple hypotheses efficiently. (ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data.
    	</div> </div>
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2021-04-07</font></h2>
					<h3><a href="https://arxiv.org/abs/1912.05656"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> VIBE: Video Inference for Human Body Pose and Shape Estimation</font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ruochen </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">	
			Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methods fail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose Video Inference for Body Pose and Shape Estimation (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape regression networks. We define a temporal network architecture and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance.
    	</div> </div>
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2021-03-31</font></h2>
					<h3><a href="https://arxiv.org/pdf/2103.03206.pdf"><font face="verdana" color="black" size="3"><U></U><font face="verdana" color="black" size="0.5"></font> Perceiver: General Perception with Iterative Attention</font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: arXiv 2021 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">	
			Biological systems understand the world by si-multaneously processing high-dimensional inputsfrom  modalities  as  diverse  as  vision,  audition,touch, proprioception, etc. The perception mod-els used in deep learning on the other hand aredesigned for individual modalities, often relyingon domain-specific assumptions such as the localgrid structures exploited by virtually all existingvision models. These priors introduce helpful in-ductive biases, but also lock models to individualmodalities.  In this paper we introducethe Per-ceiver– a model that builds upon Transformersand hence makes few architectural assumptionsabout the relationship between its inputs, but thatalso scales to hundreds of thousands of inputs,like ConvNets. The model leverages an asymmet-ric attention mechanism to iteratively distill inputsinto a tight latent bottleneck, allowing it to scale tohandle very large inputs. We show that this archi-tecture performs competitively or beyond strong,specialized models on classification tasks acrossvarious modalities: images, point clouds, audio,video and video+audio. The Perceiver obtains per-formance comparable to ResNet-50 on ImageNetwithout convolutions and by directly attending to50,000 pixels.  It also surpasses state-of-the-artresults for all modalities in AudioSet.
			    </div> </div>
	
	
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2021-03-24</font></h2>
					<h3><a href="https://ieeexplore.ieee.org/document/8569441"><font face="verdana" color="black" size="3"><U>Sensor based Prediction of Human Driving Decisions using Feed forward Neural Networks for Intelligent Vehicles</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: International Conference on Intelligent Transportation Systems 2018 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Tanvir </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">	
			Prediction of human driving decisions is an important aspect of modeling human behavior for the application to Advanced Driver Assistance Systems (ADAS) in the intelligent vehicles. This paper presents a sensor based receding horizon model for the prediction of human driving commands. Human driving decisions are expressed in terms of the vehicle speed and steering wheel angle profiles. Environmental state and human intention are the two major factors influencing the human driving decisions. The environment around the vehicle is perceived using LIDAR sensor. Feature extractor computes the occupancy grid map from the sensor data which is filtered and processed to provide precise and relevant information to the feed-forward neural network. Human intentions can be identified from the past driving decisions and represented in the form of time series data for the neural network. Supervised machine learning is used to train the neural network. Data collection and model validation is performed in the driving simulator using the SCANeR studio software. Simulation results are presented alone with the analysis.
	</div> </div>		    
		    
	
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2021-03-17</font></h2>
					<h3><a href="https://arxiv.org/pdf/1807.03146.pdf"><font face="verdana" color="black" size="3"><U>Discovery of Latent 3D Keypoints viaEnd-to-end Geometric Reasoning</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: NeurIPS 2018 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Nate </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">	
			The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.  The bestperforming models also connect the encoder and decoder through an attentionmechanism.  We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.  Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.  Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.0 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.
	</div> </div>
		    
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2021-03-10</font></h2>
					<h3><a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"><font face="verdana" color="black" size="3"><U>Attention Is All You Need</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: NeurIPS 2017 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Fengchun Qiao</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">	
			This paper presentsKeypointNet, an end-to-end geometric reasoning framework tolearnan optimal set of category-specific 3D keypoints, along with their detectors. Given a single image, KeypointNet extracts 3D keypoints that are optimized fora downstream task.  We demonstrate this framework on 3D pose estimation byproposing a differentiable objective that seeks the optimal set of keypoints forrecovering the relative pose between two views of an object. Our model discovers geometrically and semantically consistent keypoints across viewing angles andinstances of an object category. Importantly, we find that our end-to-end frameworkusing no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture on the task of pose estimation.  The discovered 3D keypoints on the car, chair, and plane categories of ShapeNet are visualized at keypointnet.github.io
			    </div> </div>
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-12-16</font></h2>
					<h3><a href="https://oops.cs.columbia.edu/"><font face="verdana" color="black" size="3"><U>OOPS! Predicting Unintentional Action in Video</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Tang Li </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">	   
		From just a short glance at a video, we can often tell whether a person’s action is intentional or not. Can we train a model to recognize this? We introduce a dataset of in-the- wild videos of unintentional action, as well as a suite of tasks for recognizing, localizing, and anticipating its onset. We train a supervised neural network as a baseline and ana- lyze its performance compared to human consistency on the tasks. We also investigate self-supervised representations that leverage natural signals in our dataset, and show the effectiveness of an approach that uses the intrinsic speed of video to perform competitively with highly-supervised pre- training. However, a significant gap between machine and human performance remains.
	    </div> </div>
		
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-12-09</font></h2>
					<h3><a href="https://arxiv.org/abs/2007.01867"><font face="verdana" color="black" size="3"><U>TLIO: Tight Learned Inertial Odometry</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: IEEE Robotics and Automation Letters 2020 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Nate </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			   In this work we propose a tightly-coupled Extended Kalman Filter framework for IMU-only state estimation. Strap-down IMU measurements provide relative state estimates based on IMU kinematic motion model. However the integration of measurements is sensitive to sensor bias and noise, causing significant drift within seconds. Recent research by Yan et al. (RoNIN) and Chen et al. (IONet) showed the capability of using trained neural networks to obtain accurate 2D displacement estimates from segments of IMU data and obtained good position estimates from concatenating them. This paper demonstrates a network that regresses 3D displacement estimates and its uncertainty, giving us the ability to tightly fuse the relative state measurement into a stochastic cloning EKF to solve for pose, velocity and sensor biases. We show that our network, trained with pedestrian data from a headset, can produce statistically consistent measurement and uncertainty to be used as the update step in the filter, and the tightly-coupled system outperforms velocity integration approaches in position estimates, and AHRS attitude filter in orientation estimates. 
	    </div> </div>	
		    
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-11-18</font></h2>
					<h3><a href="https://arxiv.org/pdf/1905.05178.pdf"><font face="verdana" color="black" size="3"><U>Graph U-Nets</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICML 2019 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Pranjal</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			   We consider the problem of representation learning for graph data. Convolutional neural networkscan  naturally  operate  on  images,  but  have  sig-nificant  challenges  in  dealing  with  graph  data.Given images are special cases of graphs withnodes lie on 2D lattices, graph embedding taskshave a natural correspondence with image pixel-wise prediction tasks such as segmentation. Whileencoder-decoder architectures like U-Nets havebeen successfully applied on many image pixel-wise prediction tasks, similar methods are lack-ing for graph data.  This is due to the fact thatpooling and up-sampling operations are not nat-ural on graph data. To address these challenges,we propose novel graph pooling (gPool) and un-pooling (gUnpool) operations in this work. ThegPool layer adaptively selects some nodes to forma smaller graph based on their scalar projectionvalues on a trainable projection vector.  We fur-ther propose the gUnpool layer as the inverse op-eration of the gPool layer.  The gUnpool layerrestores the graph into its original structure us-ing  the  position  information  of  nodes  selectedin the corresponding gPool layer.  Based on ourproposed gPool and gUnpool layers, we developan encoder-decoder model on graph, known asthe graph U-Nets.  Our experimental results onnode classification and graph classification tasksdemonstrate that our methods achieve consistentlybetter performance than previous models.
	</div> </div>	
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-11-11</font></h2>
					<h3><a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Occlusion_Aware_Unsupervised_CVPR_2018_paper.html"><font face="verdana" color="black" size="3"><U>Occlusion Aware Unsupervised Learning of Optical Flow</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2018 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ziyang Jia</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			    It has been recently shown that a convolutional neural network can learn optical flow estimation with unsuper- vised learning. However, the performance of the unsuper- vised methods still has a relatively large gap compared to its supervised counterpart. Occlusion and large motion are some of the major factors that limit the current unsuper- vised learning of optical flow methods. In this work we introduce a new method which models occlusion explicitly and a new warping way that facilitates the learning of large motion. Our method shows promising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets. Espe- cially on KITTI dataset where abundant unlabeled samples exist, our unsupervised method outperforms its counterpart trained with supervised learning.
	</div> </div>	
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-11-4</font></h2>
					<h3><a href="https://arxiv.org/pdf/1711.08585.pdf"><font face="verdana" color="black" size="3"><U>Exploiting temporal information for 3D humanpose estimation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ECCV 2018 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ruochen Wang</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			    In this work, we address the problem of 3D human pose esti-mation from a sequence of 2D human poses. Although the recent successof deep networks has led many state-of-the-art methods for 3D pose esti-mation to train deep networks end-to-end to predict from images directly,the top-performing approaches have shown the effectiveness of dividingthe task of 3D pose estimation into two steps: using a state-of-the-art 2Dpose estimator to estimate the 2D pose from images and then mappingthem into 3D space. They also showed that a low-dimensional represen-tation like 2D locations of a set of joints can be discriminative enoughto estimate 3D pose with high accuracy. However, estimation of 3D posefor individual frames leads to temporally incoherent estimates due to in-dependent error in each frame causing jitter. Therefore, in this work weutilize the temporal information across a sequence of 2D joint locationsto estimate a sequence of 3D poses. We designed a sequence-to-sequencenetwork composed of layer-normalized LSTM units with shortcut con-nections  connecting  the  input  to  the  output  on  the  decoder  side  andimposed temporal smoothness constraint during training. We found thatthe knowledge of temporal consistency improves the best reported resulton Human3.6M dataset by approximately 12.2% and helps our networkto recover temporally consistent 3D poses over a sequence of images evenwhen the 2D pose detector fails.
	</div> </div>		    
		    
		    
       <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-10-28</font></h2>
					<h3><a href="https://dl.acm.org/doi/10.1145/3394486.3403234"><font face="verdana" color="black" size="3"><U>Multimodal Learning with Incomplete Modalities by Knowledge Distillation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: KDD 2020 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng Ma</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			    Multimodal learning aims at utilizing information from a variety of data modalities to improve the generalization performance. One common approach is to seek the common information that is shared among different modalities for learning, whereas we can also fuse the supplementary information to leverage modality-specific information. Though the supplementary information is often desired, most existing multimodal approaches can only learn from samples with complete modalities, which wastes a considerable amount of data collected. Otherwise, model-based imputation needs to be used to complete the missing values and yet may introduce undesired noise, especially when the sample size is limited. In this paper, we proposed a framework based on knowledge distillation, utilizing the supplementary information from all modalities, and avoiding imputation and noise associated with it. Specifically, we first train models on each modality independently using all the available data. Then the trained models are used as teachers to teach the student model, which is trained with the samples having complete modalities. We demonstrate the effectiveness of the proposed method in extensive empirical studies on both synthetic datasets and real-world datasets.
       </div> </div>		    
		    
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-10-21</font></h2>
					<h3><a href="https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html"><font face="verdana" color="black" size="3"><U>Introduction to Self-Supervised Representation Learning</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: tutorial </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Fengchun Qiao</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			    Given a task and enough labels, supervised learning can solve it really well. Good performance usually requires a decent amount of labels, but collecting manual labels is expensive (i.e. ImageNet) and hard to be scaled up. Considering the amount of unlabelled data (e.g. free text, all the images on the Internet) is substantially more than a limited number of human curated labelled datasets, it is kinda wasteful not to use them. However, unsupervised learning is not easy and usually works much less efficiently than supervised learning.

What if we can get labels for free for unlabelled data and train unsupervised dataset in a supervised manner? We can achieve this by framing a supervised learning task in a special form to predict only a subset of information using the rest. In this way, all the information needed, both inputs and labels, has been provided. This is known as self-supervised learning.
			    </div> </div>
		    
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-10-14</font></h2>
					<h3><a href="https://arxiv.org/abs/1905.01164"><font face="verdana" color="black" size="3"><U>SinGAN: Learning a Generative Model from a Single Natural Image</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICCV 2019 (Oral) </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Tang Li</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			    We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks. 
			    </div> </div>
	
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-10-07</font></h2>
					<h3><a href="https://arxiv.org/abs/1804.00874"><font face="verdana" color="black" size="3"><U>CodeSLAM - Learning a Compact, Optimisable Representation for Dense Visual SLAM</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2018 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Nate</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			    The representation of geometry in real-time 3D perception systems continues to be a critical research issue. Dense maps capture complete surface shape and can be augmented with semantic labels, but their high dimensionality makes them computationally costly to store and process, and unsuitable for rigorous probabilistic inference. Sparse feature-based representations avoid these problems, but capture only partial scene information and are mainly useful for localisation only.
We present a new compact but dense representation of scene geometry which is conditioned on the intensity data from a single image and generated from a code consisting of a small number of parameters. We are inspired by work both on learned depth from images, and auto-encoders. Our approach is suitable for use in a keyframe-based monocular dense SLAM system: While each keyframe with a code can produce a depth map, the code can be optimised efficiently jointly with pose variables and together with the codes of overlapping keyframes to attain global consistency. Conditioning the depth map on the image allows the code to only represent aspects of the local geometry which cannot directly be predicted from the image. We explain how to learn our code representation, and demonstrate its advantageous properties in monocular SLAM. 
	</div> </div>
		    
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-09-30</font></h2>
					<h3><a href="https://dl.acm.org/doi/pdf/10.1145/3343031.3351052"><font face="verdana" color="black" size="3"><U>Unsupervised Domain Adaptation for 3D Human Pose Estimation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ACM MM 2019 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Hamed</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract:  Training an accurate 3D human pose estimator often requires a large amount of 3D ground-truth data which is inefficient and costly to collect. Previous methods have either resorted to weakly supervised methods to reduce the demand of ground-truth data for training, or using synthetically-generated but photo-realistic samples to enlarge the training data pool. Nevertheless, the former methods mainly require either additional supervision, such as unpaired 3D ground-truth data, or the camera parameters in multiview settings. On the other hand, the latter methods require accurately textured models, illumination configurations and background which need careful engineering. To address these problems, we propose a domain adaptation framework with unsupervised knowledge transfer, which aims at leveraging the knowledge in multi-modality data of the easy-to-get synthetic depth datasets to better train a pose estimator on the real-world datasets. Specifically, the framework first trains two pose estimators on synthetically-generated depth images and human body segmentation masks with full supervision, while jointly learning a human body segmentation module from the predicted 2D poses. Subsequently, the learned pose estimator and the segmentation module are applied to the real-world dataset to unsupervisedly learn a new RGB image based 2D/3D human pose estimator. Here, the knowledge encoded in the supervised learning modules are used to regularize a pose estimator without ground-truth annotations. Comprehensive experiments demonstrate significant improvements over weakly supervised methods when no ground-truth annotations are available. Further experiments with ground-truth annotations show that the proposed framework can outperform state-of-the-art fully supervised methods. In addition, we conducted ablation studies to examine the impact of each loss term, as well as with different amount of supervisions signal.</div>
	</div>    

		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-09-23</font></h2>
					<h3><a href="https://github.com/pranjal1/pytorch-gcn"><font face="verdana" color="black" size="3"><U>Introduction to Graph Convolution Networks</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: None </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Pranjal Dhakal</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			  It will consist of:
			  <ul>
			    <li> Graph Convolution operations from the following papers:</li>
			    <ul>
				<li><a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks</a></li>
				<li><a href="https://arxiv.org/abs/1706.02216">Inductive Representation Learning on Large Graphs</a></li>
				
			    </ul>
				  
			    <li> Pooling operation from the following paper: </li>
				    <ul>
				    <li><a href="https://arxiv.org/abs/1905.05178">Graph U-Nets</a></li>
			   	   </ul>
			    <li> Example network for Task 1 of the following challenge: </li>
				    <ul>
				    <li><a href="https://2015.recsyschallenge.com/challenge.html">https://2015.recsyschallenge.com/challenge.html</a></li>
			   	   </ul>
			  </ul>
			  Code and notebooks are in this <a href="https://github.com/pranjal1/pytorch-gcn">Github repo</a>.
	</div>
	</div>   
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-09-16</font></h2>
					<h3><a href="https://arxiv.org/pdf/2006.04096.pdf"><font face="verdana" color="black" size="3"><U>Robust Learning Through Cross-Task Consistency</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral) </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ziyang Jia</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract:  Visual perception entails solving a wide set of tasks, e.g.,object detection, depth estimation, etc. The predictions madefor multiple tasks from the same image are not independent,and therefore, are expected to be ‘consistent’. We propose abroadly applicable and fully computational method for aug-menting learning withCross-Task Consistency.1The pro-posed formulation is based oninference-path invarianceover a graph of arbitrary tasks.  We observe that learningwith cross-task consistency leads to more accurate predic-tions and better generalization to out-of-distribution inputs.This framework also leads to an informative unsupervisedquantity, calledConsistency Energy, based on measuringthe  intrinsic  consistency  of  the  system.   Consistency  En-ergy  correlates  well  with  the  supervised  error  (r=0.67),thus  it  can  be  employed  as  an  unsupervised  confidencemetric as well as for detection of out-of-distribution inputs(ROC-AUC=0.95). The evaluations are performed on multi-ple datasets, including Taskonomy, Replica, CocoDoom, andApolloScape,  and they benchmark cross-task consistencyversus various baselines including conventional multi-tasklearning, cycle consistency, and analytical consistency.	    
	</div>
	</div>    
		    
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-09-09</font></h2>
					<h3><a href="https://arxiv.org/abs/2004.02186"><font face="verdana" color="black" size="3"><U>Lightweight Multi-View 3D Pose Estimation through Camera-Disentangled Representation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Poster) </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ruochen Wang</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract:  We present a lightweight solution to recover 3D pose from multi-view images captured with spatially calibrated cameras. Building upon recent advances in interpretable representation learning, we exploit 3D geometry to fuse input images into a unified latent representation of pose, which is disentangled from camera view-points. This allows us to reason effectively about 3D pose across different views without using compute-intensive volumetric grids. Our architecture then conditions the learned representation on camera projection operators to produce accurate per-view 2d detections, that can be simply lifted to 3D via a differentiable Direct Linear Transform (DLT) layer. In order to do it efficiently, we propose a novel implementation of DLT that is orders of magnitude faster on GPU architectures than standard SVD-based triangulation methods. We evaluate our approach on two large-scale human pose datasets (H36M and Total Capture): our method outperforms or performs comparably to the state-of-the-art volumetric methods, while, unlike them, yielding real-time performance. </div>
		    
	</div>    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-26</font></h2>
					<h3><a href="https://arxiv.org/abs/1911.07389"><font face="verdana" color="black" size="3"><U>Towards Visually Explaining Variational Autoencoders</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral) </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Yi Liu</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: Recent advances in Convolutional Neural Network (CNN) model interpretability have led to impressive progress in visualizing and understanding model predictions. In particular, gradient-based visual attention methods have driven much recent effort in using visual attention maps as a means for visual explanations. A key problem, however, is these methods are designed for classification and categorization tasks, and their extension to explaining generative models, e.g. variational autoencoders (VAE) is not trivial. In this work, we take a step towards bridging this crucial gap, proposing the first technique to visually explain VAEs by means of gradient-based attention. We present methods to generate visual attention from the learned latent space, and also demonstrate such attention explanations serve more than just explaining VAE predictions. We show how these attention maps can be used to localize anomalies in images, demonstrating state-of-the-art performance on the MVTec-AD dataset. We also show how they can be infused into model training, helping bootstrap the VAE into learning improved latent space disentanglement, demonstrated on the Dsprites dataset. </div>
	</div>
		    
		    
	  <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-26</font></h2>
					<h3><a href="https://papers.nips.cc/paper/8346-cpm-nets-cross-partial-multi-view-networks"><font face="verdana" color="black" size="3"><U>CPM-Nets: Cross Partial Multi-View Networks</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: NeurIPS 2019 (Poster) </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng Ma</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: Despite multi-view learning progressed fast in past decades, it is still challenging due to the difficulty in modeling complex correlation among different views, especially under the context of view missing. To address the challenge, we propose a novel framework termed Cross Partial Multi-View Networks (CPM-Nets). In this framework, we first give a formal definition of completeness and versatility for multi-view representation and then theoretically prove the versatility of the latent representation learned from our algorithm. To achieve the completeness, the task of learning latent multi-view representation is specifically translated to degradation process through mimicking data transmitting, such that the optimal tradeoff between consistence and complementarity across different views could be achieved. In contrast with methods that either complete missing views or group samples according to view-missing patterns, our model fully exploits all samples and all views to produce structured representation for interpretability. Extensive experimental results validate the effectiveness of our algorithm over existing state-of-the-arts.</div>
	 </div>
		  
	 <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-19</font></h2>
					<h3><a href="https://arxiv.org/abs/2007.15553"><font face="verdana" color="black" size="3"><U>Bilevel Continual Learning</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ArXiv 2020 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Fengchun Qiao</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: Continual learning aims to learn continuously from a stream of tasks and data in an online-learning fashion, being capable of exploiting what was learned previously to improve current and future tasks while still being able to perform well on the previous tasks. One common limitation of many existing continual learning methods is that they often train a model directly on all available training data without validation due to the nature of continual learning, thus suffering poor generalization at test time. In this work, we present a novel framework of continual learning named "Bilevel Continual Learning" (BCL) by unifying a {\it bilevel optimization} objective and a {\it dual memory management} strategy comprising both episodic memory and generalization memory to achieve effective knowledge transfer to future tasks and alleviate catastrophic forgetting on old tasks simultaneously. Our extensive experiments on continual learning benchmarks demonstrate the efficacy of the proposed BCL compared to many state-of-the-art methods. Our implementation is available at https://github.com/phquang/bilevel-continual-learning. </div>
            </div>
		    
	  <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-12</font></h2>
					<h3><a href="https://www.hindawi.com/journals/wcmc/2017/9474806/"><font face="verdana" color="black" size="3"><U>Vision-Based Fall Detection with Convolutional Neural Networks</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: Wireless Communications and Mobile Computing 2017 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Hamed Fayyaz</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: One of the biggest challenges in modern societies is the improvement of healthy aging and the support to older persons in their daily activities. In particular, given its social and economic impact, the automatic detection of falls has attracted considerable attention in the computer vision and pattern recognition communities. Although the approaches based on wearable sensors have provided high detection rates, some of the potential users are reluctant to wear them and thus their use is not yet normalized. As a consequence, alternative approaches such as vision-based methods have emerged. We firmly believe that the irruption of the Smart Environments and the Internet of Things paradigms, together with the increasing number of cameras in our daily environment, forms an optimal context for vision-based systems. Consequently, here we propose a vision-based solution using Convolutional Neural Networks to decide if a sequence of frames contains a person falling. To model the video motion and make the system scenario independent, we use optical flow images as input to the networks followed by a novel three-step training phase. Furthermore, our method is evaluated in three public datasets achieving the state-of-the-art results in all three of them.</p>
	    	</div>
            </div>
		    
	   <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-05</font></h2>
					<h3><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Mohamed_Social-STGCNN_A_Social_Spatio-Temporal_Graph_Convolutional_Neural_Network_for_Human_CVPR_2020_paper.html"><font face="verdana" color="black" size="3"><U>Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Poster)</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Pranjal Dhakal</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: Better machine understanding of pedestrian behaviors enables faster progress in modeling interactions between agents such as autonomous vehicles and humans. Pedestrian trajectories are not only influenced by the pedestrian itself but also by interaction with surrounding objects. Previous methods modeled these interactions by using a variety of aggregation methods that integrate different learned pedestrians states. We propose the Social Spatio-Temporal Graph Convolutional Neural Network (Social-STGCNN), which substitutes the need of aggregation methods by modeling the interactions as a graph. Our results show an improvement over the state of art by 20% on the Final Displacement Error (FDE) and an improvement on the Average Displacement Error (ADE) with 8.5 times less parameters and up to 48 times faster inference speed than previously reported methods. In addition, our model is data efficient, and exceeds previous state of the art on the ADE metric with only 20% of the training data. We propose a kernel function to embed the social interactions between pedestrians within the adjacency matrix. Through qualitative analysis, we show that our model inherited social behaviors that can be expected between pedestrians trajectories. Code is available at https://github.com/abduallahmohamed/Social-STGCNN. 
		</p>
	    	</div>
            </div>
		    
		    
		    
		<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-07-29</font></h2>
					<h3><a href="https://arxiv.org/abs/2002.12625"><font face="verdana" color="black" size="3"><U>4D Association Graph for Realtime Multi-person Motion Capture Using Multiple Video Cameras</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ruochen Wang</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: This paper contributes a novel realtime multi-person motion capture algorithm using multiview video inputs. Due to the heavy occlusions in each view, joint optimization on the multiview images and multiple temporal frames is indispensable, which brings up the essential challenge of realtime efficiency. To this end, for the first time, we unify per-view parsing, cross-view matching, and temporal tracking into a single optimization framework, i.e., a 4D association graph that each dimension (image space, viewpoint and time) can be treated equally and simultaneously. To solve the 4D association graph efficiently, we further contribute the idea of 4D limb bundle parsing based on heuristic searching, followed with limb bundle assembling by proposing a bundle Kruskal's algorithm. Our method enables a realtime online motion capture system running at 30fps using 5 cameras on a 5-person scene. Benefiting from the unified parsing, matching and tracking constraints, our method is robust to noisy detection, and achieves high-quality online pose reconstruction quality. The proposed method outperforms the state-of-the-art method quantitatively without using high-level appearance information. We also contribute a multiview video dataset synchronized with a marker-based motion capture system for scientific evaluation. </font>
		</p>
	    	</div>
            </div>
		    
		    
		    
	    <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-07-22</font></h2>
					<h3><a href="https://arxiv.org/abs/1908.00598"><font face="verdana" color="black" size="3"><U>Sampling-free Epistemic Uncertainty Estimation Using Approximated Variance Propagation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICCV 2019 (Oral)</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ziyang jia</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: We present a sampling-free approach for computing the epistemic uncertainty of a neural network. Epistemic uncertainty is an important quantity for the deployment of deep neural networks in safety-critical applications, since it represents how much one can trust predictions on new data. Recently promising works were proposed using noise injection combined with Monte-Carlo sampling at inference time to estimate this quantity (e.g. Monte-Carlo dropout). Our main contribution is an approximation of the epistemic uncertainty estimated by these methods that does not require sampling, thus notably reducing the computational overhead. We apply our approach to large-scale visual tasks (i.e., semantic segmentation and depth regression) to demonstrate the advantages of our method compared to sampling-based approaches in terms of quality of the uncertainty estimates as well as of computational overhead. 
		    </font>
		</p>
	    	</div>
            </div>

            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-15</font></h2>
						<h3><a href="https://papers.nips.cc/paper/8777-weight-agnostic-neural-networks"><font face="verdana" color="black" size="3"><U>Weight Agnostic Neural Networks</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: NeurIPS 2019 (Poster)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Zhenzhu Zheng</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: Not all neural network architectures are created equal, some perform much better than others for certain tasks.  But how important are the weight parameters of a neural network compared to its architecture?
                                In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task.
                                We propose a search method for neural network architectures that can already perform a task without any explicit weight training.
                                To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance.
                                We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures
                                that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io
                            </font>
                        </p>
                    </div>
            </div>

            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-08</font></h2>
						<h3><a href="https://openreview.net/forum?id=Bklr3j0cKX"><font face="verdana" color="black" size="3"><U>Deep InfoMax: Learning deep representations by mutual information estimation and maximization</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICLR 2019 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Yi Liu</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals.
                            </font>
                        </p>
                    </div>
            </div>


            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-01</font></h2>
						<h3><a href="https://arxiv.org/pdf/2004.00830.pdf"><font face="verdana" color="black" size="3"><U>Tracking by Instance Detection: A Meta-Learning Approach</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng Ma</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: We consider the tracking problem as a special type of object detection problem, which we call instance detection. With proper initialization, a detector can be quickly converted into a tracker by learning the new instance from a single image. We find that model-agnostic meta-learning (MAML) offers a strategy to initialize the detector that satisfies our needs. We propose a principled three-step approach to build a high-performance tracker. First, pick any modern object detector trained with gradient descent. Second, conduct offline training (or initialization) with MAML. Third, perform domain adaptation using the initial frame. We follow this procedure to build two trackers, named Retina-MAML and FCOS-MAML, based on two modern detectors RetinaNet and FCOS. Evaluations on four benchmarks show that both trackers are competitive against state-of-the-art trackers. On OTB-100, Retina-MAML achieves the highest ever AUC of 0.712. On TrackingNet, FCOS-MAML ranks the first on the leader board with an AUC of 0.757 and the normalized precision of 0.822. Both trackers run in real-time at 40 FPS.
                            </font>
                        </p>
                    </div>
            </div>


            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-06-24</font></h2>
						<h3><a href="https://liuziwei7.github.io/projects/CompoundDomain.html"><font face="verdana" color="black" size="3"><U>Open Compound Domain Adaptation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Fengchun Qiao</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: A typical domain adaptation approach is to adapt models trained on the annotated data in a source domain (e.g., sunny weather) for achieving high performance on the test data in a target domain (e.g., rainy weather). Whether the target contains a single homogeneous domain or multiple heterogeneous domains, existing works always assume that there exist clear distinctions between the domains, which is often not true in practice (e.g., changes in weather). We study an open compound domain adaptation (OCDA) problem, where the target is a compound of multiple homogeneous domains without domain labels, reflecting realistic data collection from mixed and novel situations. We propose a new approach based on two technical insights into OCDA: 1) a curriculum domain adaptation strategy to bootstrap generalization across domains in a data-driven self-organizing fashion and 2) a memory module to increase the model’s agility towards novel domains. Our experiments on digit classification, facial expression recognition, semantic segmentation, and reinforcement learning demonstrate the effectiveness of our approach.
                            </font>
                        </p>
                    </div>
            </div>


            </div>
            <div class="side">
                <div class="author_info">
                    <div class="author_descri">
                        <h4><font face="verdana" color="black" size="4">About Reading Group</font></h4>

                                <p align="left"><font face="verdana" color="black" size="2.8">The reading group is held weekly by <a href="https://sites.google.com/site/xipengcshomepage/research?authuser=0"> <font face="verdana" color="black" size="2.8"> <U> D-REAL </U> </font> </a> at University of Delaware. The goal is to broaden the scope of research interest in <i>Machine Learning</i>, <i>Deep Learning</i>, and <i>Computer Vision</i> by sharing and discussing high-quality papers. </font></p>
                                <!-- <br/><p align="left"><font face="verdana" color="black" size="2.8">Each presenter shall provide the link of paper and presentation slides at least ten hours before meeting.</font></p> -->
                                <br/><p align="left"><font face="verdana" color="black" size="2.8">Please feel free to contact Qitong (wqtwjt at udel dot edu) if you want to join the reading group (presentation is highly welcome but not required).</font></p>

                    </div>
                </div>

                <div class="top_article">
                    <h3><font face="verdana" color="black" size="4">Schedule at Spring 2022</font></h3>
                    <ul>
                        <li><font face="verdana" color="black" size="2.8">Qitong Wang: Jan. 20th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Meng Ma: Jan. 27th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Fengchun Qiao: Feb. 3rd </font></li>
						<li><font face="verdana" color="black" size="2.8">Tang Li: Feb. 8th </font></li>
						<li><font face="verdana" color="black" size="2.8">Kien Nguyen: Feb. 15th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Nathaniel Merrill: Feb. 22nd </font></li>
                        <li><font face="verdana" color="black" size="2.8">Amani Arman Kiruga: Mar. 1st </font></li>
						<!-- <li><font face="verdana" color="black" size="2.8">Wenxuan Li: Mar. 8th </font></li> -->
						<li><font face="verdana" color="black" size="2.8">Ziyang Jia: Mar. 15th </font></li>
						<li><font face="verdana" color="black" size="2.8">Qitong Wang: Mar. 22nd </font></li>
                        <li><font face="verdana" color="black" size="2.8">Meng Ma: Apr. 5th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Fengchun Qiao: Apr. 12th </font></li>
						<li><font face="verdana" color="black" size="2.8">Tang Li: Apr. 19th </font></li>
						<li><font face="verdana" color="black" size="2.8">Kien Nguyen: Apr. 26th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Nathaniel Merrill: May 3rd </font></li>
                        <!-- <li><font face="verdana" color="black" size="2.8">Amani Arman Kiruga: Mar. 1st </font></li> -->
						<!-- <li><font face="verdana" color="black" size="2.8">Wenxuan Li: Mar. 8th </font></li> -->
						<li><font face="verdana" color="black" size="2.8">Ziyang Jia: May 10th </font></li>
			<!-- <li><font face="verdana" color="black" size="2.8">Tang : May 5th </font></li> -->
			<!-- <li><font face="verdana" color="black" size="2.8">Qitong : May 12th </font></li> -->
                    </ul>
                </div>

                <div class="top_article">
                    <h3><font face="verdana" color="black" size="4">Contact</font></h3>
                    <ul>
                        <li><font face="verdana" color="black" size="2.8">Person: Qitong Wang</font></li>
						<li><font face="verdana" color="black" size="2.8">Email: wqtwjt at udel dot edu</font>
                    </ul>
                </div>

                <div class="top_article">
                    <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=v5s2FYX0VY2D6mH83Q6FIVJ3kgFtRY0IogxL89fjLAg&cl=ffffff&w=a"></script>
                </div>

            </div>

            <div class="clearfloat"></div>
        </div>

    </body>
</html>
