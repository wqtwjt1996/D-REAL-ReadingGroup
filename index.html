<!doctype html>
<html lang="zh-CN">
    <head>
        <meta charset="ansi">
	<meta name="description" content="this is a website">
        <meta name="keywords" content="website,html,css">

        <title>ReadingGroup</title>

        <link rel="stylesheet" style="text/css" href="index.css">

    </head>
    <body>
        <div id="head">
            <div class="logo_title">
                <h1 style = "text-align:center"><font face="verdana" color="#0D1E91">Deep Robust & Explainable AI Lab Reading Group</font></h1>
          </div>
            <div class="clearfloat"></div>
        </div>

        <div id="wrapper">
            <div class="main">


            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-15</font></h2>
						<h3><a href="https://papers.nips.cc/paper/8777-weight-agnostic-neural-networks"><font face="verdana" color="black" size="3"><U>Weight Agnostic Neural Networks</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: NeurIPS 2019 (Poster)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Zhenzhu Zheng</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: Not all neural network architectures are created equal, some perform much betterthan others for certain tasks.  But how important are the weight parameters of aneural network compared to its architecture?  In this work, we question to whatextent neural network architectures alone, without learning any weight parameters,can encode solutions for a given task.  We propose a search method for neuralnetwork architectures that can already perform a task without any explicit weighttraining. To evaluate these networks, we populate the connections with a singleshared weight parameter sampled from a uniform random distribution, and measurethe expected performance. We demonstrate that our method can find minimal neuralnetwork architectures that can perform several reinforcement learning tasks withoutweight training. On a supervised learning domain, we find network architecturesthat achieve much higher than chance accuracy on MNIST using random weights.Interactive version of this paper athttps://weightagnostic.github.io
                            </font>
                        </p>
                    </div>
            </div>

            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-08</font></h2>
						<h3><a href="https://openreview.net/forum?id=Bklr3j0cKX"><font face="verdana" color="black" size="3"><U>Deep InfoMax: Learning deep representations by mutual information estimation and maximization</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICLR 2019 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Yi Liu</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals.
                            </font>
                        </p>
                    </div>
            </div>


            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-01</font></h2>
						<h3><a href="https://arxiv.org/pdf/2004.00830.pdf"><font face="verdana" color="black" size="3"><U>Tracking by Instance Detection: A Meta-Learning Approach</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng Ma</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: We consider the tracking problem as a special type of object detection problem, which we call instance detection. With proper initialization, a detector can be quickly converted into a tracker by learning the new instance from a single image. We find that model-agnostic meta-learning (MAML) offers a strategy to initialize the detector that satisfies our needs. We propose a principled three-step approach to build a high-performance tracker. First, pick any modern object detector trained with gradient descent. Second, conduct offline training (or initialization) with MAML. Third, perform domain adaptation using the initial frame. We follow this procedure to build two trackers, named Retina-MAML and FCOS-MAML, based on two modern detectors RetinaNet and FCOS. Evaluations on four benchmarks show that both trackers are competitive against state-of-the-art trackers. On OTB-100, Retina-MAML achieves the highest ever AUC of 0.712. On TrackingNet, FCOS-MAML ranks the first on the leader board with an AUC of 0.757 and the normalized precision of 0.822. Both trackers run in real-time at 40 FPS.
                            </font>
                        </p>
                    </div>
            </div>


            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-06-24</font></h2>
						<h3><a href="https://liuziwei7.github.io/projects/CompoundDomain.html"><font face="verdana" color="black" size="3"><U>Open Compound Domain Adaptation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Fengchun Qiao</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: A typical domain adaptation approach is to adapt models trained on the annotated data in a source domain (e.g., sunny weather) for achieving high performance on the test data in a target domain (e.g., rainy weather). Whether the target contains a single homogeneous domain or multiple heterogeneous domains, existing works always assume that there exist clear distinctions between the domains, which is often not true in practice (e.g., changes in weather). We study an open compound domain adaptation (OCDA) problem, where the target is a compound of multiple homogeneous domains without domain labels, reflecting realistic data collection from mixed and novel situations. We propose a new approach based on two technical insights into OCDA: 1) a curriculum domain adaptation strategy to bootstrap generalization across domains in a data-driven self-organizing fashion and 2) a memory module to increase the modelâ€™s agility towards novel domains. Our experiments on digit classification, facial expression recognition, semantic segmentation, and reinforcement learning demonstrate the effectiveness of our approach.
                            </font>
                        </p>
                    </div>
            </div>


            </div>
            <div class="side">
                <div class="author_info">
                    <div class="author_descri">
                        <h4><font face="verdana" color="black" size="4">About Reading Group</font></h4>

                                <p align="left"><font face="verdana" color="black" size="2.8">The reading group is held weekly by <a href="https://sites.google.com/site/xipengcshomepage/research?authuser=0"> <font face="verdana" color="black" size="2.8"> <U> D-REAL </U> </font> </a> at University of Delaware. The goal is to broaden the scope of research interest in <i>Machine Learning</i>, <i>Deep Learning</i>, and <i>Computer Vision</i> by sharing and discussing high-quality papers. </font></p>
                                <br/><p align="left"><font face="verdana" color="black" size="2.8">Each presenter shall provide the link of paper and presentation slides at least ten hours before meeting.</font></p>
                                <br/><p align="left"><font face="verdana" color="black" size="2.8">Please feel free to contact Fengchun (fengchun at udel dot edu) if you want to join the reading group (presentation is highly welcome but not required).</font></p>

                    </div>
                </div>

                <div class="top_article">
                    <h3><font face="verdana" color="black" size="4">Schedule</font></h3>
                    <ul>
                        <li><font face="verdana" color="black" size="2.8">Fengchun: June 24th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Meng: July 1st </font></li>
                        <li><font face="verdana" color="black" size="2.8">Yi: July 8th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Zhenzhu: July 15th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Ziyang: July 22nd </font></li>
                        <li><font face="verdana" color="black" size="2.8">Ruochen: July 29th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Pranjal: August 5th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Yujie : August 12th </font></li>
                    </ul>
                </div>

                <div class="top_article">
                    <h3><font face="verdana" color="black" size="4">Contact</font></h3>
                    <ul>
                        <li><font face="verdana" color="black" size="2.8">Person: Fengchun Qiao</font></li>
						<li><font face="verdana" color="black" size="2.8">Email: fengchun at udel dot edu</font>
                    </ul>
                </div>

                <div class="top_article">
                    <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=v5s2FYX0VY2D6mH83Q6FIVJ3kgFtRY0IogxL89fjLAg&cl=ffffff&w=a"></script>
                </div>

            </div>

            <div class="clearfloat"></div>
        </div>

    </body>
</html>
