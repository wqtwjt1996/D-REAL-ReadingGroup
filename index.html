<!doctype html>
<html lang="zh-CN">
    <head>
        <meta charset="ansi">
	<meta name="description" content="this is a website">
        <meta name="keywords" content="website,html,css">

        <title>ReadingGroup</title>

        <link rel="stylesheet" style="text/css" href="index.css">

    </head>
    <body>
        <div id="head">
            <div class="logo_title">
                <h1 style = "text-align:center"><font face="verdana" color="#0D1E91">Deep Robust & Explainable AI Lab Reading Group</font></h1>
          </div>
            <div class="clearfloat"></div>
        </div>

        <div id="wrapper">
            <div class="main">
	
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-12-16</font></h2>
					<h3><a href="https://oops.cs.columbia.edu/"><font face="verdana" color="black" size="3"><U>OOPS! Predicting Unintentional Action in Video</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Tang Li </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">	   
		From just a short glance at a video, we can often tell whether a person’s action is intentional or not. Can we train a model to recognize this? We introduce a dataset of in-the- wild videos of unintentional action, as well as a suite of tasks for recognizing, localizing, and anticipating its onset. We train a supervised neural network as a baseline and ana- lyze its performance compared to human consistency on the tasks. We also investigate self-supervised representations that leverage natural signals in our dataset, and show the effectiveness of an approach that uses the intrinsic speed of video to perform competitively with highly-supervised pre- training. However, a significant gap between machine and human performance remains.
	    </div> </div>
		
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-12-09</font></h2>
					<h3><a href="https://arxiv.org/abs/2007.01867"><font face="verdana" color="black" size="3"><U>TLIO: Tight Learned Inertial Odometry</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: IEEE Robotics and Automation Letters 2020 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Nate </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			   In this work we propose a tightly-coupled Extended Kalman Filter framework for IMU-only state estimation. Strap-down IMU measurements provide relative state estimates based on IMU kinematic motion model. However the integration of measurements is sensitive to sensor bias and noise, causing significant drift within seconds. Recent research by Yan et al. (RoNIN) and Chen et al. (IONet) showed the capability of using trained neural networks to obtain accurate 2D displacement estimates from segments of IMU data and obtained good position estimates from concatenating them. This paper demonstrates a network that regresses 3D displacement estimates and its uncertainty, giving us the ability to tightly fuse the relative state measurement into a stochastic cloning EKF to solve for pose, velocity and sensor biases. We show that our network, trained with pedestrian data from a headset, can produce statistically consistent measurement and uncertainty to be used as the update step in the filter, and the tightly-coupled system outperforms velocity integration approaches in position estimates, and AHRS attitude filter in orientation estimates. 
	    </div> </div>	
		    
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-11-18</font></h2>
					<h3><a href="https://arxiv.org/pdf/1905.05178.pdf"><font face="verdana" color="black" size="3"><U>Graph U-Nets</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICML 2019 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Pranjal</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			   We consider the problem of representation learning for graph data. Convolutional neural networkscan  naturally  operate  on  images,  but  have  sig-nificant  challenges  in  dealing  with  graph  data.Given images are special cases of graphs withnodes lie on 2D lattices, graph embedding taskshave a natural correspondence with image pixel-wise prediction tasks such as segmentation. Whileencoder-decoder architectures like U-Nets havebeen successfully applied on many image pixel-wise prediction tasks, similar methods are lack-ing for graph data.  This is due to the fact thatpooling and up-sampling operations are not nat-ural on graph data. To address these challenges,we propose novel graph pooling (gPool) and un-pooling (gUnpool) operations in this work. ThegPool layer adaptively selects some nodes to forma smaller graph based on their scalar projectionvalues on a trainable projection vector.  We fur-ther propose the gUnpool layer as the inverse op-eration of the gPool layer.  The gUnpool layerrestores the graph into its original structure us-ing  the  position  information  of  nodes  selectedin the corresponding gPool layer.  Based on ourproposed gPool and gUnpool layers, we developan encoder-decoder model on graph, known asthe graph U-Nets.  Our experimental results onnode classification and graph classification tasksdemonstrate that our methods achieve consistentlybetter performance than previous models.
	</div> </div>	
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-11-11</font></h2>
					<h3><a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Occlusion_Aware_Unsupervised_CVPR_2018_paper.html"><font face="verdana" color="black" size="3"><U>Occlusion Aware Unsupervised Learning of Optical Flow</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2018 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ziyang Jia</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			    It has been recently shown that a convolutional neural network can learn optical flow estimation with unsuper- vised learning. However, the performance of the unsuper- vised methods still has a relatively large gap compared to its supervised counterpart. Occlusion and large motion are some of the major factors that limit the current unsuper- vised learning of optical flow methods. In this work we introduce a new method which models occlusion explicitly and a new warping way that facilitates the learning of large motion. Our method shows promising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets. Espe- cially on KITTI dataset where abundant unlabeled samples exist, our unsupervised method outperforms its counterpart trained with supervised learning.
	</div> </div>	
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-11-4</font></h2>
					<h3><a href="https://arxiv.org/pdf/1711.08585.pdf"><font face="verdana" color="black" size="3"><U>Exploiting temporal information for 3D humanpose estimation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ECCV 2018 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ruochen Wang</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			    In this work, we address the problem of 3D human pose esti-mation from a sequence of 2D human poses. Although the recent successof deep networks has led many state-of-the-art methods for 3D pose esti-mation to train deep networks end-to-end to predict from images directly,the top-performing approaches have shown the effectiveness of dividingthe task of 3D pose estimation into two steps: using a state-of-the-art 2Dpose estimator to estimate the 2D pose from images and then mappingthem into 3D space. They also showed that a low-dimensional represen-tation like 2D locations of a set of joints can be discriminative enoughto estimate 3D pose with high accuracy. However, estimation of 3D posefor individual frames leads to temporally incoherent estimates due to in-dependent error in each frame causing jitter. Therefore, in this work weutilize the temporal information across a sequence of 2D joint locationsto estimate a sequence of 3D poses. We designed a sequence-to-sequencenetwork composed of layer-normalized LSTM units with shortcut con-nections  connecting  the  input  to  the  output  on  the  decoder  side  andimposed temporal smoothness constraint during training. We found thatthe knowledge of temporal consistency improves the best reported resulton Human3.6M dataset by approximately 12.2% and helps our networkto recover temporally consistent 3D poses over a sequence of images evenwhen the 2D pose detector fails.
	</div> </div>		    
		    
		    
       <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-10-28</font></h2>
					<h3><a href="https://dl.acm.org/doi/10.1145/3394486.3403234"><font face="verdana" color="black" size="3"><U>Multimodal Learning with Incomplete Modalities by Knowledge Distillation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: KDD 2020 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng Ma</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			    Multimodal learning aims at utilizing information from a variety of data modalities to improve the generalization performance. One common approach is to seek the common information that is shared among different modalities for learning, whereas we can also fuse the supplementary information to leverage modality-specific information. Though the supplementary information is often desired, most existing multimodal approaches can only learn from samples with complete modalities, which wastes a considerable amount of data collected. Otherwise, model-based imputation needs to be used to complete the missing values and yet may introduce undesired noise, especially when the sample size is limited. In this paper, we proposed a framework based on knowledge distillation, utilizing the supplementary information from all modalities, and avoiding imputation and noise associated with it. Specifically, we first train models on each modality independently using all the available data. Then the trained models are used as teachers to teach the student model, which is trained with the samples having complete modalities. We demonstrate the effectiveness of the proposed method in extensive empirical studies on both synthetic datasets and real-world datasets.
       </div> </div>		    
		    
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-10-21</font></h2>
					<h3><a href="https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html"><font face="verdana" color="black" size="3"><U>Introduction to Self-Supervised Representation Learning</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: tutorial </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Fengchun Qiao</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			    Given a task and enough labels, supervised learning can solve it really well. Good performance usually requires a decent amount of labels, but collecting manual labels is expensive (i.e. ImageNet) and hard to be scaled up. Considering the amount of unlabelled data (e.g. free text, all the images on the Internet) is substantially more than a limited number of human curated labelled datasets, it is kinda wasteful not to use them. However, unsupervised learning is not easy and usually works much less efficiently than supervised learning.

What if we can get labels for free for unlabelled data and train unsupervised dataset in a supervised manner? We can achieve this by framing a supervised learning task in a special form to predict only a subset of information using the rest. In this way, all the information needed, both inputs and labels, has been provided. This is known as self-supervised learning.
			    </div> </div>
		    
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-10-14</font></h2>
					<h3><a href="https://arxiv.org/abs/1905.01164"><font face="verdana" color="black" size="3"><U>SinGAN: Learning a Generative Model from a Single Natural Image</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICCV 2019 (Oral) </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Tang Li</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			    We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks. 
			    </div> </div>
	
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-10-07</font></h2>
					<h3><a href="https://arxiv.org/abs/1804.00874"><font face="verdana" color="black" size="3"><U>CodeSLAM - Learning a Compact, Optimisable Representation for Dense Visual SLAM</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2018 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Nate</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			    The representation of geometry in real-time 3D perception systems continues to be a critical research issue. Dense maps capture complete surface shape and can be augmented with semantic labels, but their high dimensionality makes them computationally costly to store and process, and unsuitable for rigorous probabilistic inference. Sparse feature-based representations avoid these problems, but capture only partial scene information and are mainly useful for localisation only.
We present a new compact but dense representation of scene geometry which is conditioned on the intensity data from a single image and generated from a code consisting of a small number of parameters. We are inspired by work both on learned depth from images, and auto-encoders. Our approach is suitable for use in a keyframe-based monocular dense SLAM system: While each keyframe with a code can produce a depth map, the code can be optimised efficiently jointly with pose variables and together with the codes of overlapping keyframes to attain global consistency. Conditioning the depth map on the image allows the code to only represent aspects of the local geometry which cannot directly be predicted from the image. We explain how to learn our code representation, and demonstrate its advantageous properties in monocular SLAM. 
	</div> </div>
		    
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-09-30</font></h2>
					<h3><a href="https://dl.acm.org/doi/pdf/10.1145/3343031.3351052"><font face="verdana" color="black" size="3"><U>Unsupervised Domain Adaptation for 3D Human Pose Estimation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ACM MM 2019 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Hamed</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract:  Training an accurate 3D human pose estimator often requires a large amount of 3D ground-truth data which is inefficient and costly to collect. Previous methods have either resorted to weakly supervised methods to reduce the demand of ground-truth data for training, or using synthetically-generated but photo-realistic samples to enlarge the training data pool. Nevertheless, the former methods mainly require either additional supervision, such as unpaired 3D ground-truth data, or the camera parameters in multiview settings. On the other hand, the latter methods require accurately textured models, illumination configurations and background which need careful engineering. To address these problems, we propose a domain adaptation framework with unsupervised knowledge transfer, which aims at leveraging the knowledge in multi-modality data of the easy-to-get synthetic depth datasets to better train a pose estimator on the real-world datasets. Specifically, the framework first trains two pose estimators on synthetically-generated depth images and human body segmentation masks with full supervision, while jointly learning a human body segmentation module from the predicted 2D poses. Subsequently, the learned pose estimator and the segmentation module are applied to the real-world dataset to unsupervisedly learn a new RGB image based 2D/3D human pose estimator. Here, the knowledge encoded in the supervised learning modules are used to regularize a pose estimator without ground-truth annotations. Comprehensive experiments demonstrate significant improvements over weakly supervised methods when no ground-truth annotations are available. Further experiments with ground-truth annotations show that the proposed framework can outperform state-of-the-art fully supervised methods. In addition, we conducted ablation studies to examine the impact of each loss term, as well as with different amount of supervisions signal.</div>
	</div>    

		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-09-23</font></h2>
					<h3><a href="https://github.com/pranjal1/pytorch-gcn"><font face="verdana" color="black" size="3"><U>Introduction to Graph Convolution Networks</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: None </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Pranjal Dhakal</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
			  It will consist of:
			  <ul>
			    <li> Graph Convolution operations from the following papers:</li>
			    <ul>
				<li><a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks</a></li>
				<li><a href="https://arxiv.org/abs/1706.02216">Inductive Representation Learning on Large Graphs</a></li>
				
			    </ul>
				  
			    <li> Pooling operation from the following paper: </li>
				    <ul>
				    <li><a href="https://arxiv.org/abs/1905.05178">Graph U-Nets</a></li>
			   	   </ul>
			    <li> Example network for Task 1 of the following challenge: </li>
				    <ul>
				    <li><a href="https://2015.recsyschallenge.com/challenge.html">https://2015.recsyschallenge.com/challenge.html</a></li>
			   	   </ul>
			  </ul>
			  Code and notebooks are in this <a href="https://github.com/pranjal1/pytorch-gcn">Github repo</a>.
	</div>
	</div>   
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-09-16</font></h2>
					<h3><a href="https://arxiv.org/pdf/2006.04096.pdf"><font face="verdana" color="black" size="3"><U>Robust Learning Through Cross-Task Consistency</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral) </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ziyang Jia</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract:  Visual perception entails solving a wide set of tasks, e.g.,object detection, depth estimation, etc. The predictions madefor multiple tasks from the same image are not independent,and therefore, are expected to be ‘consistent’. We propose abroadly applicable and fully computational method for aug-menting learning withCross-Task Consistency.1The pro-posed formulation is based oninference-path invarianceover a graph of arbitrary tasks.  We observe that learningwith cross-task consistency leads to more accurate predic-tions and better generalization to out-of-distribution inputs.This framework also leads to an informative unsupervisedquantity, calledConsistency Energy, based on measuringthe  intrinsic  consistency  of  the  system.   Consistency  En-ergy  correlates  well  with  the  supervised  error  (r=0.67),thus  it  can  be  employed  as  an  unsupervised  confidencemetric as well as for detection of out-of-distribution inputs(ROC-AUC=0.95). The evaluations are performed on multi-ple datasets, including Taskonomy, Replica, CocoDoom, andApolloScape,  and they benchmark cross-task consistencyversus various baselines including conventional multi-tasklearning, cycle consistency, and analytical consistency.	    
	</div>
	</div>    
		    
		    
		    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-09-09</font></h2>
					<h3><a href="https://arxiv.org/abs/2004.02186"><font face="verdana" color="black" size="3"><U>Lightweight Multi-View 3D Pose Estimation through Camera-Disentangled Representation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Poster) </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ruochen Wang</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract:  We present a lightweight solution to recover 3D pose from multi-view images captured with spatially calibrated cameras. Building upon recent advances in interpretable representation learning, we exploit 3D geometry to fuse input images into a unified latent representation of pose, which is disentangled from camera view-points. This allows us to reason effectively about 3D pose across different views without using compute-intensive volumetric grids. Our architecture then conditions the learned representation on camera projection operators to produce accurate per-view 2d detections, that can be simply lifted to 3D via a differentiable Direct Linear Transform (DLT) layer. In order to do it efficiently, we propose a novel implementation of DLT that is orders of magnitude faster on GPU architectures than standard SVD-based triangulation methods. We evaluate our approach on two large-scale human pose datasets (H36M and Total Capture): our method outperforms or performs comparably to the state-of-the-art volumetric methods, while, unlike them, yielding real-time performance. </div>
		    
	</div>    
	<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-26</font></h2>
					<h3><a href="https://arxiv.org/abs/1911.07389"><font face="verdana" color="black" size="3"><U>Towards Visually Explaining Variational Autoencoders</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral) </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Yi Liu</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: Recent advances in Convolutional Neural Network (CNN) model interpretability have led to impressive progress in visualizing and understanding model predictions. In particular, gradient-based visual attention methods have driven much recent effort in using visual attention maps as a means for visual explanations. A key problem, however, is these methods are designed for classification and categorization tasks, and their extension to explaining generative models, e.g. variational autoencoders (VAE) is not trivial. In this work, we take a step towards bridging this crucial gap, proposing the first technique to visually explain VAEs by means of gradient-based attention. We present methods to generate visual attention from the learned latent space, and also demonstrate such attention explanations serve more than just explaining VAE predictions. We show how these attention maps can be used to localize anomalies in images, demonstrating state-of-the-art performance on the MVTec-AD dataset. We also show how they can be infused into model training, helping bootstrap the VAE into learning improved latent space disentanglement, demonstrated on the Dsprites dataset. </div>
	</div>
		    
		    
	  <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-26</font></h2>
					<h3><a href="https://papers.nips.cc/paper/8346-cpm-nets-cross-partial-multi-view-networks"><font face="verdana" color="black" size="3"><U>CPM-Nets: Cross Partial Multi-View Networks</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: NeurIPS 2019 (Poster) </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng Ma</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: Despite multi-view learning progressed fast in past decades, it is still challenging due to the difficulty in modeling complex correlation among different views, especially under the context of view missing. To address the challenge, we propose a novel framework termed Cross Partial Multi-View Networks (CPM-Nets). In this framework, we first give a formal definition of completeness and versatility for multi-view representation and then theoretically prove the versatility of the latent representation learned from our algorithm. To achieve the completeness, the task of learning latent multi-view representation is specifically translated to degradation process through mimicking data transmitting, such that the optimal tradeoff between consistence and complementarity across different views could be achieved. In contrast with methods that either complete missing views or group samples according to view-missing patterns, our model fully exploits all samples and all views to produce structured representation for interpretability. Extensive experimental results validate the effectiveness of our algorithm over existing state-of-the-arts.</div>
	 </div>
		  
	 <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-19</font></h2>
					<h3><a href="https://arxiv.org/abs/2007.15553"><font face="verdana" color="black" size="3"><U>Bilevel Continual Learning</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ArXiv 2020 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Fengchun Qiao</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: Continual learning aims to learn continuously from a stream of tasks and data in an online-learning fashion, being capable of exploiting what was learned previously to improve current and future tasks while still being able to perform well on the previous tasks. One common limitation of many existing continual learning methods is that they often train a model directly on all available training data without validation due to the nature of continual learning, thus suffering poor generalization at test time. In this work, we present a novel framework of continual learning named "Bilevel Continual Learning" (BCL) by unifying a {\it bilevel optimization} objective and a {\it dual memory management} strategy comprising both episodic memory and generalization memory to achieve effective knowledge transfer to future tasks and alleviate catastrophic forgetting on old tasks simultaneously. Our extensive experiments on continual learning benchmarks demonstrate the efficacy of the proposed BCL compared to many state-of-the-art methods. Our implementation is available at https://github.com/phquang/bilevel-continual-learning. </div>
            </div>
		    
	  <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-12</font></h2>
					<h3><a href="https://www.hindawi.com/journals/wcmc/2017/9474806/"><font face="verdana" color="black" size="3"><U>Vision-Based Fall Detection with Convolutional Neural Networks</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: Wireless Communications and Mobile Computing 2017 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Hamed Fayyaz</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: One of the biggest challenges in modern societies is the improvement of healthy aging and the support to older persons in their daily activities. In particular, given its social and economic impact, the automatic detection of falls has attracted considerable attention in the computer vision and pattern recognition communities. Although the approaches based on wearable sensors have provided high detection rates, some of the potential users are reluctant to wear them and thus their use is not yet normalized. As a consequence, alternative approaches such as vision-based methods have emerged. We firmly believe that the irruption of the Smart Environments and the Internet of Things paradigms, together with the increasing number of cameras in our daily environment, forms an optimal context for vision-based systems. Consequently, here we propose a vision-based solution using Convolutional Neural Networks to decide if a sequence of frames contains a person falling. To model the video motion and make the system scenario independent, we use optical flow images as input to the networks followed by a novel three-step training phase. Furthermore, our method is evaluated in three public datasets achieving the state-of-the-art results in all three of them.</p>
	    	</div>
            </div>
		    
	   <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-05</font></h2>
					<h3><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Mohamed_Social-STGCNN_A_Social_Spatio-Temporal_Graph_Convolutional_Neural_Network_for_Human_CVPR_2020_paper.html"><font face="verdana" color="black" size="3"><U>Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Poster)</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Pranjal Dhakal</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: Better machine understanding of pedestrian behaviors enables faster progress in modeling interactions between agents such as autonomous vehicles and humans. Pedestrian trajectories are not only influenced by the pedestrian itself but also by interaction with surrounding objects. Previous methods modeled these interactions by using a variety of aggregation methods that integrate different learned pedestrians states. We propose the Social Spatio-Temporal Graph Convolutional Neural Network (Social-STGCNN), which substitutes the need of aggregation methods by modeling the interactions as a graph. Our results show an improvement over the state of art by 20% on the Final Displacement Error (FDE) and an improvement on the Average Displacement Error (ADE) with 8.5 times less parameters and up to 48 times faster inference speed than previously reported methods. In addition, our model is data efficient, and exceeds previous state of the art on the ADE metric with only 20% of the training data. We propose a kernel function to embed the social interactions between pedestrians within the adjacency matrix. Through qualitative analysis, we show that our model inherited social behaviors that can be expected between pedestrians trajectories. Code is available at https://github.com/abduallahmohamed/Social-STGCNN. 
		</p>
	    	</div>
            </div>
		    
		    
		    
		<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-07-29</font></h2>
					<h3><a href="https://arxiv.org/abs/2002.12625"><font face="verdana" color="black" size="3"><U>4D Association Graph for Realtime Multi-person Motion Capture Using Multiple Video Cameras</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ruochen Wang</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: This paper contributes a novel realtime multi-person motion capture algorithm using multiview video inputs. Due to the heavy occlusions in each view, joint optimization on the multiview images and multiple temporal frames is indispensable, which brings up the essential challenge of realtime efficiency. To this end, for the first time, we unify per-view parsing, cross-view matching, and temporal tracking into a single optimization framework, i.e., a 4D association graph that each dimension (image space, viewpoint and time) can be treated equally and simultaneously. To solve the 4D association graph efficiently, we further contribute the idea of 4D limb bundle parsing based on heuristic searching, followed with limb bundle assembling by proposing a bundle Kruskal's algorithm. Our method enables a realtime online motion capture system running at 30fps using 5 cameras on a 5-person scene. Benefiting from the unified parsing, matching and tracking constraints, our method is robust to noisy detection, and achieves high-quality online pose reconstruction quality. The proposed method outperforms the state-of-the-art method quantitatively without using high-level appearance information. We also contribute a multiview video dataset synchronized with a marker-based motion capture system for scientific evaluation. </font>
		</p>
	    	</div>
            </div>
		    
		    
		    
	    <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-07-22</font></h2>
					<h3><a href="https://arxiv.org/abs/1908.00598"><font face="verdana" color="black" size="3"><U>Sampling-free Epistemic Uncertainty Estimation Using Approximated Variance Propagation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICCV 2019 (Oral)</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ziyang jia</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: We present a sampling-free approach for computing the epistemic uncertainty of a neural network. Epistemic uncertainty is an important quantity for the deployment of deep neural networks in safety-critical applications, since it represents how much one can trust predictions on new data. Recently promising works were proposed using noise injection combined with Monte-Carlo sampling at inference time to estimate this quantity (e.g. Monte-Carlo dropout). Our main contribution is an approximation of the epistemic uncertainty estimated by these methods that does not require sampling, thus notably reducing the computational overhead. We apply our approach to large-scale visual tasks (i.e., semantic segmentation and depth regression) to demonstrate the advantages of our method compared to sampling-based approaches in terms of quality of the uncertainty estimates as well as of computational overhead. 
		    </font>
		</p>
	    	</div>
            </div>

            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-15</font></h2>
						<h3><a href="https://papers.nips.cc/paper/8777-weight-agnostic-neural-networks"><font face="verdana" color="black" size="3"><U>Weight Agnostic Neural Networks</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: NeurIPS 2019 (Poster)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Zhenzhu Zheng</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: Not all neural network architectures are created equal, some perform much better than others for certain tasks.  But how important are the weight parameters of a neural network compared to its architecture?
                                In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task.
                                We propose a search method for neural network architectures that can already perform a task without any explicit weight training.
                                To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance.
                                We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures
                                that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io
                            </font>
                        </p>
                    </div>
            </div>

            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-08</font></h2>
						<h3><a href="https://openreview.net/forum?id=Bklr3j0cKX"><font face="verdana" color="black" size="3"><U>Deep InfoMax: Learning deep representations by mutual information estimation and maximization</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICLR 2019 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Yi Liu</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals.
                            </font>
                        </p>
                    </div>
            </div>


            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-01</font></h2>
						<h3><a href="https://arxiv.org/pdf/2004.00830.pdf"><font face="verdana" color="black" size="3"><U>Tracking by Instance Detection: A Meta-Learning Approach</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng Ma</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: We consider the tracking problem as a special type of object detection problem, which we call instance detection. With proper initialization, a detector can be quickly converted into a tracker by learning the new instance from a single image. We find that model-agnostic meta-learning (MAML) offers a strategy to initialize the detector that satisfies our needs. We propose a principled three-step approach to build a high-performance tracker. First, pick any modern object detector trained with gradient descent. Second, conduct offline training (or initialization) with MAML. Third, perform domain adaptation using the initial frame. We follow this procedure to build two trackers, named Retina-MAML and FCOS-MAML, based on two modern detectors RetinaNet and FCOS. Evaluations on four benchmarks show that both trackers are competitive against state-of-the-art trackers. On OTB-100, Retina-MAML achieves the highest ever AUC of 0.712. On TrackingNet, FCOS-MAML ranks the first on the leader board with an AUC of 0.757 and the normalized precision of 0.822. Both trackers run in real-time at 40 FPS.
                            </font>
                        </p>
                    </div>
            </div>


            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-06-24</font></h2>
						<h3><a href="https://liuziwei7.github.io/projects/CompoundDomain.html"><font face="verdana" color="black" size="3"><U>Open Compound Domain Adaptation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Fengchun Qiao</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: A typical domain adaptation approach is to adapt models trained on the annotated data in a source domain (e.g., sunny weather) for achieving high performance on the test data in a target domain (e.g., rainy weather). Whether the target contains a single homogeneous domain or multiple heterogeneous domains, existing works always assume that there exist clear distinctions between the domains, which is often not true in practice (e.g., changes in weather). We study an open compound domain adaptation (OCDA) problem, where the target is a compound of multiple homogeneous domains without domain labels, reflecting realistic data collection from mixed and novel situations. We propose a new approach based on two technical insights into OCDA: 1) a curriculum domain adaptation strategy to bootstrap generalization across domains in a data-driven self-organizing fashion and 2) a memory module to increase the model’s agility towards novel domains. Our experiments on digit classification, facial expression recognition, semantic segmentation, and reinforcement learning demonstrate the effectiveness of our approach.
                            </font>
                        </p>
                    </div>
            </div>


            </div>
            <div class="side">
                <div class="author_info">
                    <div class="author_descri">
                        <h4><font face="verdana" color="black" size="4">About Reading Group</font></h4>

                                <p align="left"><font face="verdana" color="black" size="2.8">The reading group is held weekly by <a href="https://sites.google.com/site/xipengcshomepage/research?authuser=0"> <font face="verdana" color="black" size="2.8"> <U> D-REAL </U> </font> </a> at University of Delaware. The goal is to broaden the scope of research interest in <i>Machine Learning</i>, <i>Deep Learning</i>, and <i>Computer Vision</i> by sharing and discussing high-quality papers. </font></p>
                                <br/><p align="left"><font face="verdana" color="black" size="2.8">Each presenter shall provide the link of paper and presentation slides at least ten hours before meeting.</font></p>
                                <br/><p align="left"><font face="verdana" color="black" size="2.8">Please feel free to contact Fengchun (fengchun at udel dot edu) if you want to join the reading group (presentation is highly welcome but not required).</font></p>

                    </div>
                </div>

                <div class="top_article">
                    <h3><font face="verdana" color="black" size="4">Schedule</font></h3>
                    <ul>
                        <li><font face="verdana" color="black" size="2.8">Fengchun: October 21st </font></li>
                        <li><font face="verdana" color="black" size="2.8">Meng: October 28th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Ruochen: November 4th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Ziyang: November 11th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Pranjal: November 18th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Nate : December 9th </font></li>
			<li><font face="verdana" color="black" size="2.8">Tang : December 16th </font></li>
                    </ul>
                </div>

                <div class="top_article">
                    <h3><font face="verdana" color="black" size="4">Contact</font></h3>
                    <ul>
                        <li><font face="verdana" color="black" size="2.8">Person: Fengchun Qiao</font></li>
						<li><font face="verdana" color="black" size="2.8">Email: fengchun at udel dot edu</font>
                    </ul>
                </div>

                <div class="top_article">
                    <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=v5s2FYX0VY2D6mH83Q6FIVJ3kgFtRY0IogxL89fjLAg&cl=ffffff&w=a"></script>
                </div>

            </div>

            <div class="clearfloat"></div>
        </div>

    </body>
</html>
